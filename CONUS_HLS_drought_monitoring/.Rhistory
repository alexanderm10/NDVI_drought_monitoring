ndvi_file <- file.path(hls_paths$processed_ndvi, "test", paste0(scene_id, "_NDVI.tif"))
dir.create(dirname(ndvi_file), recursive = TRUE, showWarnings = FALSE)
# Calculate NDVI
red_raster <- rast(red_file)
nir_raster <- rast(nir_file)
ndvi <- (nir_raster - red_raster) / (nir_raster + red_raster)
writeRaster(ndvi, ndvi_file, overwrite = TRUE)
cat("✓ NDVI calculation successful\n")
cat("Files created:\n")
cat("  Red band:", red_file, "\n")
cat("  NIR band:", nir_file, "\n")
cat("  NDVI:", ndvi_file, "\n")
return(TRUE)
}
return(FALSE)
}
test_hls_search()
run_full_test()
# Debug the API response structure
library(httr)
library(jsonlite)
debug_hls_response <- function() {
cat("=== DEBUGGING HLS API RESPONSE ===\n")
# Make the same request as the failing function
stac_url <- "https://cmr.earthdata.nasa.gov/stac/LPCLOUD/search"
chicago_bbox <- c(-88.5, 41.5, -87.5, 42.0)
query_params <- list(
collections = "HLSL30.v2.0",
bbox = paste(chicago_bbox, collapse = ","),
datetime = "2024-07-01T00:00:00Z/2024-07-15T23:59:59Z",
limit = 5  # Just get a few for debugging
)
cat("Making API request...\n")
response <- GET(
url = stac_url,
query = query_params,
add_headers(
"Accept" = "application/json",
"User-Agent" = "R/httr HLS-Data-Access"
),
timeout(60)
)
cat("Status code:", status_code(response), "\n")
if (status_code(response) != 200) {
cat("API call failed\n")
return(NULL)
}
# Get raw text response
content_text <- content(response, "text", encoding = "UTF-8")
cat("Raw response length:", nchar(content_text), "characters\n")
# Show first part of response
cat("First 500 characters of response:\n")
cat(substr(content_text, 1, 500), "\n\n")
# Try to parse JSON
cat("Attempting to parse JSON...\n")
content_json <- try(fromJSON(content_text), silent = TRUE)
if (inherits(content_json, "try-error")) {
cat("❌ JSON parsing failed:\n")
print(content_json)
return(NULL)
}
cat("✓ JSON parsing successful\n")
# Examine structure
cat("Top-level structure:\n")
cat("Names:", paste(names(content_json), collapse = ", "), "\n")
cat("Type:", class(content_json), "\n")
# Check features
if ("features" %in% names(content_json)) {
features <- content_json$features
cat("Features found:", length(features), "\n")
cat("Features type:", class(features), "\n")
if (length(features) > 0) {
cat("First feature type:", class(features[[1]]), "\n")
cat("First feature names:", paste(names(features[[1]]), collapse = ", "), "\n")
# Check properties specifically
if ("properties" %in% names(features[[1]])) {
props <- features[[1]]$properties
cat("Properties type:", class(props), "\n")
if (is.list(props)) {
cat("Properties names:", paste(names(props), collapse = ", "), "\n")
# Look for cloud cover
if ("eo:cloud_cover" %in% names(props)) {
cat("Cloud cover:", props$`eo:cloud_cover`, "\n")
} else {
cat("Available property keys containing 'cloud':\n")
cloud_keys <- names(props)[grepl("cloud", names(props), ignore.case = TRUE)]
cat(paste(cloud_keys, collapse = ", "), "\n")
}
}
}
# Check assets
if ("assets" %in% names(features[[1]])) {
assets <- features[[1]]$assets
cat("Assets type:", class(assets), "\n")
if (is.list(assets)) {
cat("Available assets:", paste(names(assets), collapse = ", "), "\n")
}
}
}
} else {
cat("No 'features' found in response\n")
cat("Available keys:", paste(names(content_json), collapse = ", "), "\n")
}
return(content_json)
}
# Run the debug
cat("Running response structure debug...\n")
result <- debug_hls_response()
head(result)
source("03_working_hls_acquisition.R")
quick_search_test()
bbox
start_date
search_hls_working <- function(bbox, start_date, end_date, cloud_cover = 50, max_items = 100) {
cat("Searching HLS data:\n")
cat("  Bbox:", paste(bbox, collapse = ", "), "\n")
cat("  Date range:", start_date, "to", end_date, "\n")
cat("  Max cloud cover:", cloud_cover, "%\n")
# NASA CMR-STAC endpoint
stac_url <- "https://cmr.earthdata.nasa.gov/stac/LPCLOUD/search"
query_params <- list(
collections = "HLSL30.v2.0",
bbox = paste(bbox, collapse = ","),
datetime = paste0(start_date, "T00:00:00Z/", end_date, "T23:59:59Z"),
limit = max_items
)
cat("Making API request...\n")
response <- try({
GET(
url = stac_url,
query = query_params,
add_headers(
"Accept" = "application/json",
"User-Agent" = "R/httr HLS-Data-Access"
),
timeout(60)
)
}, silent = TRUE)
if (inherits(response, "try-error")) {
cat("❌ API request failed\n")
return(data.frame())
}
if (status_code(response) != 200) {
cat("❌ API request failed with status:", status_code(response), "\n")
return(data.frame())
}
# Parse response
content_text <- content(response, "text", encoding = "UTF-8")
content_json <- try(fromJSON(content_text), silent = TRUE)
if (inherits(content_json, "try-error")) {
cat("❌ Failed to parse JSON response\n")
return(data.frame())
}
if (is.null(content_json$features) || nrow(content_json$features) == 0) {
cat("⚠ No features found in response\n")
return(data.frame())
}
# Get the flattened features data frame
features_df <- content_json$features
# Filter by cloud cover (column name is "properties.eo:cloud_cover")
cloud_col <- "properties.eo:cloud_cover"
if (cloud_col %in% names(features_df)) {
# Remove rows with NA cloud cover or > threshold
valid_rows <- !is.na(features_df[[cloud_col]]) & features_df[[cloud_col]] <= cloud_cover
features_df <- features_df[valid_rows, ]
cat("✓ Found", nrow(features_df), "scenes with cloud cover ≤", cloud_cover, "%\n")
} else {
cat("⚠ Cloud cover column not found, keeping all scenes\n")
}
# Check that we have the required bands
required_bands <- c("assets.B04.href", "assets.B05.href")
missing_bands <- required_bands[!required_bands %in% names(features_df)]
if (length(missing_bands) > 0) {
cat("❌ Missing required bands:", paste(missing_bands, collapse = ", "), "\n")
return(data.frame())
}
cat("✓ All required bands available (B04-Red, B05-NIR)\n")
return(features_df)
}
quick_search_test()
chicago_bbox <- c(-88.2, 41.7, -87.8, 42.1)
results <- search_hls_working(
bbox = chicago_bbox,
start_date = "2024-07-01",
end_date = "2024-07-15",
cloud_cover = 50
)
bbox = chicago_bbox
results <- search_hls_working(
bbox = chicago_bbox,
start_date = "2024-07-01",
end_date = "2024-07-15",
cloud_cover = 50
)
start_date = "2024-07-01"
end_date = "2024-07-15"
cloud_cover = 50
stac_url <- "https://cmr.earthdata.nasa.gov/stac/LPCLOUD/search"
query_params <- list(
collections = "HLSL30.v2.0",
bbox = paste(bbox, collapse = ","),
datetime = paste0(start_date, "T00:00:00Z/", end_date, "T23:59:59Z"),
limit = max_items
)
max_items=100
# NASA CMR-STAC endpoint
stac_url <- "https://cmr.earthdata.nasa.gov/stac/LPCLOUD/search"
query_params <- list(
collections = "HLSL30.v2.0",
bbox = paste(bbox, collapse = ","),
datetime = paste0(start_date, "T00:00:00Z/", end_date, "T23:59:59Z"),
limit = max_items
)
response <- try({
GET(
url = stac_url,
query = query_params,
add_headers(
"Accept" = "application/json",
"User-Agent" = "R/httr HLS-Data-Access"
),
timeout(60)
)
}, silent = TRUE)
response
# Parse response
content_text <- content(response, "text", encoding = "UTF-8")
content_json <- try(fromJSON(content_text), silent = TRUE)
content_json
View(content_json)
dirname(rprojroot::find_rstudio_root_file())
setwd("~/r_files/drought_grant/NDVI_drought_monitoring/CONUS_HLS_drought_monitoring/NASA_R_tutorial")
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = NA)
knitr::opts_knit$set(root.dir = dirname(rprojroot::find_rstudio_root_file()))
roi <- terra::vect("data/Field_Boundary.geojson")
setwd("~/r_files/drought_grant/NDVI_drought_monitoring/CONUS_HLS_drought_monitoring")
source("debug_columns.R")
source("04_final_working_hls.R")
quick_search_test_final()
source("01_HLS_data_acquisition.R")
# Test the complete workflow
test_hls_pipeline()
source("02_operational_hls_pipeline.R")
# Test the complete workflow
test_fixed_pipeline()
source("03_nasa_auth_fixed.R")
# Test the complete workflow
test_nasa_auth_methods()
source("01_HLS_data_acquisition_FINAL.R")
test_hls_pipeline()
source("01_HLS_data_acquisition_FINAL.R")
test_hls_pipeline()
source("debug_dual_collections.R")
source("01_HLS_data_acquisition_FINAL.R")
test_hls_pipeline()
source("02_midwest_pilot.R")
test_midwest_search()
r.Home()
R.home()
library(lubridate)
# Source required scripts
source("00_setup_paths.R")
source("01_HLS_data_acquisition_FINAL.R")
source("02_midwest_pilot.R")
# Data pull configuration
DATA_PULL_CONFIG <- list(
# Domain settings
domain = "NIDIS_Midwest_DEWS",
# Time range settings
start_year = 2020,          # Start with recent years first
end_year = 2024,            # Up to current year
current_year_cutoff = "2024-10-31",  # Don't process incomplete current year data
# Quality settings
cloud_cover_max = 40,       # Slightly higher for better temporal coverage
# Processing settings
batch_size_years = 1,       # Process one year at a time
retry_failed_downloads = TRUE,
# Storage settings
storage_path = hls_paths$base,
backup_logs = TRUE
)
# Initialize or load execution status
initialize_execution_status <- function(config = DATA_PULL_CONFIG) {
status_file <- file.path(hls_paths$logs, "execution_status.json")
if (file.exists(status_file)) {
cat("Loading existing execution status...\n")
status <- jsonlite::fromJSON(status_file)
} else {
cat("Initializing new execution status...\n")
status <- list(
execution_id = paste0("MIDWEST_", format(Sys.time(), "%Y%m%d_%H%M%S")),
start_time = Sys.time(),
domain = config$domain,
total_years = config$end_year - config$start_year + 1,
years_completed = character(0),
years_failed = character(0),
current_year = NULL,
total_scenes_found = 0,
total_scenes_downloaded = 0,
total_ndvi_processed = 0,
total_data_gb = 0,
last_update = Sys.time(),
status = "initialized"
)
}
return(status)
}
# Save execution status
save_execution_status <- function(status) {
status_file <- file.path(hls_paths$logs, "execution_status.json")
dir.create(dirname(status_file), recursive = TRUE, showWarnings = FALSE)
status$last_update <- Sys.time()
jsonlite::write_json(status, status_file, pretty = TRUE, auto_unbox = TRUE)
cat("Execution status saved to:", status_file, "\n")
}
# Display execution status
display_status <- function(status) {
cat("\n=== EXECUTION STATUS ===\n")
cat("Execution ID:", status$execution_id, "\n")
cat("Domain:", status$domain, "\n")
cat("Progress:", length(status$years_completed), "/", status$total_years, "years completed\n")
cat("Current status:", status$status, "\n")
if (length(status$years_completed) > 0) {
cat("Completed years:", paste(status$years_completed, collapse = ", "), "\n")
}
if (length(status$years_failed) > 0) {
cat("Failed years:", paste(status$years_failed, collapse = ", "), "\n")
}
cat("Total scenes found:", status$total_scenes_found, "\n")
cat("Total scenes downloaded:", status$total_scenes_downloaded, "\n")
cat("Total NDVI processed:", status$total_ndvi_processed, "\n")
cat("Estimated data volume:", round(status$total_data_gb, 1), "GB\n")
cat("Last update:", as.character(status$last_update), "\n")
cat("========================\n\n")
}
run_preflight_checks <- function(config = DATA_PULL_CONFIG) {
cat("=== PRE-FLIGHT CHECKS ===\n")
checks_passed <- 0
total_checks <- 6
# Check 1: NASA authentication
cat("1. Checking NASA Earthdata authentication...\n")
auth_result <- try(create_nasa_session(), silent = TRUE)
if (!inherits(auth_result, "try-error")) {
cat("   ✓ NASA authentication working\n")
checks_passed <- checks_passed + 1
} else {
cat("   ❌ NASA authentication failed\n")
cat("   Please check your _netrc file\n")
}
# Check 2: Storage space
cat("2. Checking storage space...\n")
tryCatch({
check_storage_space(config$storage_path)
cat("   ✓ Storage path accessible\n")
checks_passed <- checks_passed + 1
}, error = function(e) {
cat("   ❌ Storage check failed:", e$message, "\n")
})
# Check 3: Directory structure
cat("3. Checking directory structure...\n")
create_hls_directory_structure(hls_paths)
cat("   ✓ Directory structure ready\n")
checks_passed <- checks_passed + 1
# Check 4: API connectivity
cat("4. Testing NASA API connectivity...\n")
api_test <- test_midwest_search(year = 2024, month = 7)
if (api_test) {
cat("   ✓ NASA API accessible\n")
checks_passed <- checks_passed + 1
} else {
cat("   ❌ NASA API test failed\n")
}
# Check 5: Download test
cat("5. Testing download functionality...\n")
download_test <- test_hls_pipeline()
if (download_test) {
cat("   ✓ Download pipeline working\n")
checks_passed <- checks_passed + 1
} else {
cat("   ❌ Download test failed\n")
}
# Check 6: Time range validation
cat("6. Validating time range...\n")
if (config$start_year >= 2013 && config$end_year <= year(Sys.Date())) {
cat("   ✓ Time range valid (", config$start_year, "-", config$end_year, ")\n")
checks_passed <- checks_passed + 1
} else {
cat("   ❌ Invalid time range\n")
}
# Summary
cat("\nPRE-FLIGHT SUMMARY: ", checks_passed, "/", total_checks, " checks passed\n")
if (checks_passed == total_checks) {
cat("🎉 ALL CHECKS PASSED - Ready for data acquisition!\n\n")
return(TRUE)
} else {
cat("❌ CHECKS FAILED - Please resolve issues before proceeding\n\n")
return(FALSE)
}
}
execute_midwest_data_pull <- function(config = DATA_PULL_CONFIG,
run_preflight = TRUE,
resume_from_status = TRUE) {
cat("=== NIDIS MIDWEST DEWS DATA PULL EXECUTION ===\n")
cat("Execution started at:", as.character(Sys.time()), "\n\n")
# Run pre-flight checks
if (run_preflight) {
if (!run_preflight_checks(config)) {
stop("Pre-flight checks failed. Aborting execution.")
}
}
# Initialize or load status
status <- initialize_execution_status(config)
display_status(status)
# Determine years to process
all_years <- config$start_year:config$end_year
if (resume_from_status && length(status$years_completed) > 0) {
remaining_years <- all_years[!all_years %in% as.numeric(status$years_completed)]
cat("Resuming execution. Remaining years:", paste(remaining_years, collapse = ", "), "\n\n")
} else {
remaining_years <- all_years
cat("Starting fresh execution for years:", paste(remaining_years, collapse = ", "), "\n\n")
}
if (length(remaining_years) == 0) {
cat("✅ All years already completed!\n")
return(status)
}
status$status <- "running"
save_execution_status(status)
# Process each year
for (year in remaining_years) {
cat("=== PROCESSING YEAR", year, "===\n")
status$current_year <- year
status$status <- paste("processing_year", year)
save_execution_status(status)
# Run year-specific acquisition
year_result <- try({
acquire_midwest_pilot_data(
start_year = year,
end_year = year,
cloud_cover_max = config$cloud_cover_max
)
}, silent = FALSE)
if (!inherits(year_result, "try-error")) {
# Year completed successfully
status$years_completed <- c(status$years_completed, as.character(year))
status$total_scenes_found <- status$total_scenes_found + year_result$total_scenes_found
status$total_scenes_downloaded <- status$total_scenes_downloaded + year_result$total_scenes_downloaded
status$total_ndvi_processed <- status$total_ndvi_processed + year_result$total_ndvi_processed
# Estimate data volume (rough)
year_gb <- (year_result$total_scenes_downloaded * 2 * 15 + year_result$total_ndvi_processed * 15) / 1024
status$total_data_gb <- status$total_data_gb + year_gb
cat("✅ Year", year, "completed successfully\n")
cat("   Scenes downloaded:", year_result$total_scenes_downloaded, "\n")
cat("   NDVI processed:", year_result$total_ndvi_processed, "\n\n")
} else {
# Year failed
status$years_failed <- c(status$years_failed, as.character(year))
cat("❌ Year", year, "failed with error:\n")
print(year_result)
cat("\n")
}
status$current_year <- NULL
save_execution_status(status)
display_status(status)
}
# Final status
if (length(status$years_failed) == 0) {
status$status <- "completed"
cat("🎉 EXECUTION COMPLETED SUCCESSFULLY!\n")
} else {
status$status <- "completed_with_errors"
cat("⚠ EXECUTION COMPLETED WITH ERRORS\n")
cat("Failed years:", paste(status$years_failed, collapse = ", "), "\n")
}
status$end_time <- Sys.time()
execution_time <- difftime(status$end_time, status$start_time, units = "hours")
cat("Total execution time:", round(as.numeric(execution_time), 2), "hours\n")
save_execution_status(status)
display_status(status)
return(status)
}
# Quick status check
check_execution_status <- function() {
status_file <- file.path(hls_paths$logs, "execution_status.json")
if (file.exists(status_file)) {
status <- jsonlite::fromJSON(status_file)
display_status(status)
return(status)
} else {
cat("No execution status found. Run execute_midwest_data_pull() to start.\n")
return(NULL)
}
}
# Resume failed execution
resume_execution <- function() {
cat("Resuming from last saved status...\n")
execute_midwest_data_pull(resume_from_status = TRUE, run_preflight = FALSE)
}
# Reset execution (start fresh)
reset_execution <- function() {
status_file <- file.path(hls_paths$logs, "execution_status.json")
if (file.exists(status_file)) {
backup_file <- paste0(status_file, ".backup_", format(Sys.time(), "%Y%m%d_%H%M%S"))
file.copy(status_file, backup_file)
file.remove(status_file)
cat("Previous execution status backed up to:", backup_file, "\n")
}
cat("Execution reset. Run execute_midwest_data_pull() to start fresh.\n")
}
run_preflight_checks()
execute_midwest_data_pull()
